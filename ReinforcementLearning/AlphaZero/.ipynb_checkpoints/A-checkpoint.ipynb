{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2652b290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22.2\n",
      "1.12.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(np.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cda48d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return - player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a04a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlock, num_hidden):\n",
    "        super().__init__()\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlock)]\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8fdd19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0. -1.  0.]]\n",
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[1. 1. 0.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 0. 1.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n",
      "0.3571953773498535 [0.10998861 0.04107455 0.18166003 0.1483578  0.10509919 0.13030697\n",
      " 0.17063838 0.03711835 0.07575613]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tictactoe = TicTacToe()\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, 1)\n",
    "state = tictactoe.get_next_state(state, 7, -1)\n",
    "\n",
    "print(state)\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "\n",
    "print(encoded_state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value, policy)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c33a6424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        # self.expandable_moves = game.get_valid_moves(state)\n",
    "        \n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "        # return np.sum(self.expandable_moves) == 0 and len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "        # return q_value + self.args['C'] * math.sqrt(math.log(self.visit_count) / child.visit_count)\n",
    "    \n",
    "    def expand(self, policy):  # expansion\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                \n",
    "                # action = np.random.choice(np.where(self.expandable_moves == 1)[0])\n",
    "                # self.expandable_moves[action] = 0\n",
    "\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player = -1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                \n",
    "        return child\n",
    "    \n",
    "#     def simulate(self):\n",
    "#         value, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)\n",
    "#         value = self.game.get_opponent_value(value)\n",
    "        \n",
    "#         if is_terminal:\n",
    "#             return value\n",
    "        \n",
    "#         rollout_state = self.state.copy()\n",
    "#         rollout_player = 1\n",
    "#         while True:\n",
    "#             valid_moves = self.game.get_valid_moves(rollout_state)\n",
    "#             action = np.random.choice(np.where(valid_moves == 1)[0])\n",
    "#             rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n",
    "#             value, is_terminal = self.game.get_value_and_terminated(rollout_state, action)\n",
    "#             if is_terminal:\n",
    "#                 if rollout_player == -1:\n",
    "#                     value = self.game.get_opponent_value(value)\n",
    "#                 return value\n",
    "\n",
    "#             rollout_player = self.game.get_opponent(rollout_player)\n",
    "    \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n",
    "    \n",
    "    \n",
    "    \n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state)).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                # value = node.simulate()\n",
    "                \n",
    "                node.expand(policy)\n",
    "                \n",
    "            node.backpropagate(value)\n",
    "            \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c76d72bb-972e-4ac3-bbb6-973fbc56defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    return memory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "            \n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in range(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "                \n",
    "            torch.save(self.model.state_dict(), f'model_{iteration}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f'optimizer_{iteration}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1acc0c0c-697d-4785-8a49-8211270d8d5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_searches\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m60\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     13\u001b[0m }\n\u001b[0;32m     15\u001b[0m alphaZero \u001b[38;5;241m=\u001b[39m AlphaZero(model, optimizer, tictactoe, args)\n\u001b[1;32m---> 16\u001b[0m \u001b[43malphaZero\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mAlphaZero.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m selfPlay_iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_selfPlay_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 45\u001b[0m     memory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselfPlay()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iterations': 3,\n",
    "    'num_selfPlay_iterations': 10,\n",
    "    'num_epochs': 4,\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, tictactoe, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56f63df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "valid_moves [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0. -1.]]\n",
      "valid_moves [0, 1, 2, 3, 5, 6, 7]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0. -1.]]\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0. -1. -1.]]\n",
      "valid_moves [0, 2, 3, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1. -1. -1.]]\n",
      "[[ 0.  1. -1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1. -1. -1.]]\n",
      "valid_moves [0, 3, 5]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1. -1.]\n",
      " [ 0.  1.  1.]\n",
      " [ 1. -1. -1.]]\n",
      "[[ 0.  1. -1.]\n",
      " [-1.  1.  1.]\n",
      " [ 1. -1. -1.]]\n",
      "valid_moves [0]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1. -1.]\n",
      " [-1.  1.  1.]\n",
      " [ 1. -1. -1.]]\n",
      "draw\n"
     ]
    }
   ],
   "source": [
    "tictactoe = TicTacToe()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'C': 1.41,\n",
    "    'num_searches': 1000,\n",
    "}\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(tictactoe, args, model)\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        \n",
    "        valid_moves = tictactoe.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "    else:\n",
    "        neutral_state = tictactoe.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "    \n",
    "    \n",
    "    state = tictactoe.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = tictactoe.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = tictactoe.get_opponent(player)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbaead7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e6a74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54de350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e4eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2412f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
