{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1.0, -2.0, 3.0]\n",
    "w = [-3.0, -1.0, 2.0]\n",
    "b = 1.0\n",
    "\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = max(z, 0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvalue = 1.0\n",
    "drelu_dz = 1. if z > 0 else 0\n",
    "drelu_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0 # 1*1\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "print(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 1.0 -1.0\n"
     ]
    }
   ],
   "source": [
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dmul_dx0 = w[0]\n",
    "dmul_dx1 = w[1]\n",
    "dmul_dx2 = w[2]\n",
    "dmul_dw0 = x[0]\n",
    "dmul_dw1 = x[1]\n",
    "dmul_dw2 = x[2]\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1            \n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2\n",
    "\n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.0, 2.0] 1.0\n"
     ]
    }
   ],
   "source": [
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w[0] += -0.001 * dw[0]\n",
    "# w[1] += -0.001 * dw[1]\n",
    "# w[2] += -0.001 * dw[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# a vector of 1s\n",
    "dvalues = np.array([[1., 1., 1.]])\n",
    "dvalues[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2 ,  0.5 , -0.26],\n",
       "       [ 0.8 , -0.91, -0.27],\n",
       "       [-0.5 ,  0.26,  0.17],\n",
       "       [ 1.  , -0.5 ,  0.87]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44, -0.38, -0.07,  1.37])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum weights related to the given input multiplied by\n",
    "# the gradient related to the given neuron\n",
    "dx0 = sum([weights[0][0]*dvalues[0][0], weights[0][1]*dvalues[0][1],\n",
    "            weights[0][2]*dvalues[0][2]])\n",
    "dx1 = sum([weights[1][0]*dvalues[0][0], weights[1][1]*dvalues[0][1],\n",
    "            weights[1][2]*dvalues[0][2]])\n",
    "dx2 = sum([weights[2][0]*dvalues[0][0], weights[2][1]*dvalues[0][1],\n",
    "            weights[2][2]*dvalues[0][2]])\n",
    "dx3 = sum([weights[3][0]*dvalues[0][0], weights[3][1]*dvalues[0][1],\n",
    "            weights[3][2]*dvalues[0][2]])\n",
    "\n",
    "dinputs = np.array([dx0, dx1, dx2, dx3])\n",
    "dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# a vector of 1s\n",
    "dvalues = np.array([[1., 1., 1.]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "# print(weights)\n",
    "# sum weights of given input\n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "dinputs = np.dot(dvalues[0], weights.T)\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "# sum weights of given input\n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "dinputs = np.dot(dvalues, weights.T)\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                    [2., 5., -1., 2],\n",
    "                    [-1.5, 2.7, 3.3, -0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "# sum weights of given input\n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims\n",
    "# since this by default will produce a plain list -\n",
    "# we explained this in the chapter 4\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "print(dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "[[1 1 0 0]\n",
      " [1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "# Example layer output\n",
    "z = np.array([[1, 2, -3, -4],\n",
    "                [2, -7, -1, 3],\n",
    "                [-1, 2, 5, -1]])\n",
    "dvalues = np.array([[1, 2, 3, 4],\n",
    "                    [5, 6, 7, 8],\n",
    "                    [9, 10, 11, 12]])\n",
    "\n",
    "# ReLU activation's derivative\n",
    "drelu = np.zeros_like(dvalues)\n",
    "print(drelu)\n",
    "drelu[z > 0] = 1\n",
    "print(drelu)\n",
    "\n",
    "# The chain rule\n",
    "drelu *= dvalues\n",
    "print(drelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "# Example layer output\n",
    "z = np.array([[1, 2, -3, -4],\n",
    "                [2, -7, -1, 3],\n",
    "                [-1, 2, 5, -1]])\n",
    "dvalues = np.array([[1, 2, 3, 4],\n",
    "                    [5, 6, 7, 8],\n",
    "                    [9, 10, 11, 12]])\n",
    "\n",
    "# ReLU activation's derivative\n",
    "# with the chain rule applied\n",
    "drelu = dvalues.copy()\n",
    "drelu[z <= 0] = 0\n",
    "print(drelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                    [2., 5., -1., 2],\n",
    "                    [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.179515   0.5003665 -0.262746 ]\n",
      " [ 0.742093  -0.9152577 -0.2758402]\n",
      " [-0.510153   0.2529017  0.1629592]\n",
      " [ 0.971328  -0.5021842  0.8636583]]\n",
      "[[1.98489  2.997739 0.497389]]\n"
     ]
    }
   ],
   "source": [
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# Forward pass\n",
    "layer_outputs = np.dot(inputs, weights) + biases # Dense layer\n",
    "relu_outputs = np.maximum(0, layer_outputs) # ReLU activation\n",
    "\n",
    "# Let's optimize and test backpropagation here\n",
    "# ReLU activation - simulates derivative with respect to input values\n",
    "# from next layer passed to current layer during backpropagation\n",
    "drelu = relu_outputs.copy()\n",
    "drelu[layer_outputs <= 0] = 0\n",
    "\n",
    "# Dense layer\n",
    "# dinputs - multiply by weights\n",
    "dinputs = np.dot(drelu, weights.T)\n",
    "\n",
    "# dweights - multiply by inputs\n",
    "dweights = np.dot(inputs.T, drelu)\n",
    "\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims\n",
    "# since this by default will produce a plain list -\n",
    "# we explained this in the chapter 4\n",
    "dbiases = np.sum(drelu, axis=0, keepdims=True)\n",
    "\n",
    "# Update parameters\n",
    "weights += -0.001 * dweights\n",
    "biases += -0.001 * dbiases\n",
    "print(weights)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, inputs, neurons):\n",
    "        self.weights = 0.01 * np.random.randn(inputs, neurons)\n",
    "        self.biases = np.zeros((1, neurons))\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    \n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "        \n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    ...\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        ...\n",
    "        self.inputs = inputs\n",
    "\n",
    "        \n",
    "class Layer_Dense:\n",
    "    ...\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "    \n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable,\n",
    "        # let's make a copy of the values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
