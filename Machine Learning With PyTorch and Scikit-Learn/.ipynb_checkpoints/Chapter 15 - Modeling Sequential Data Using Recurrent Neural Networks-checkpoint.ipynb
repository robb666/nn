{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: Parameter containing:\n",
      "tensor([[ 0.3643, -0.3121, -0.1371,  0.3319, -0.6657],\n",
      "        [ 0.4241, -0.1455,  0.3597,  0.0983, -0.0866]], requires_grad=True)\n",
      "\n",
      "W_xh shape: torch.Size([2, 5])\n",
      "W_hh shape: torch.Size([2, 2])\n",
      "b_xh shape: torch.Size([2])\n",
      "b_hh shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "rnn_layer = nn.RNN(input_size=5, hidden_size=2,\n",
    "                   num_layers=1, batch_first=True)\n",
    "w_xh = rnn_layer.weight_ih_l0\n",
    "w_hh = rnn_layer.weight_hh_l0\n",
    "b_xh = rnn_layer.bias_ih_l0\n",
    "b_hh = rnn_layer.bias_hh_l0\n",
    "\n",
    "print('W_xh shape:', w_xh)\n",
    "print()\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_hh shape:', w_hh.shape)\n",
    "print('b_xh shape:', b_xh.shape)\n",
    "print('b_hh shape:', b_hh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "    Input           : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden           : [[-0.3161478   0.64722455]]\n",
      "    Output (manual) : [[-0.21046415  0.56788784]]\n",
      "    RNN output      : [[-0.3519801   0.52525216]]\n",
      "\n",
      "Time step 1 =>\n",
      "    Input           : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden           : [[-0.73478645  1.2972739 ]]\n",
      "    Output (manual) : [[-0.5741978  0.7945334]]\n",
      "    RNN output      : [[-0.68424344  0.76074266]]\n",
      "\n",
      "Time step 2 =>\n",
      "    Input           : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden           : [[-1.153425   1.9473232]]\n",
      "    Output (manual) : [[-0.8130059   0.91817397]]\n",
      "    RNN output      : [[-0.8649416   0.90466356]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
    "\n",
    "## output of the simple RNN:\n",
    "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n",
    "\n",
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1, 5))\n",
    "    print(f'Time step {t} =>')\n",
    "    print('    Input           :', xt.numpy())\n",
    "    \n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_hh\n",
    "    print('   Hidden           :', ht.detach().numpy())\n",
    "    if t > 0:\n",
    "         prev_h = out_man[t-1]\n",
    "    else:\n",
    "         prev_h = torch.zeros((ht.shape))\n",
    "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) \\\n",
    "            + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('    Output (manual) :', ot.detach().numpy())\n",
    "    print('    RNN output      :', output[:, t].detach().numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(x_seq[t], (1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_seq[[t]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Project one â€“ predicting the sentiment of IMDb movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "from collections.abc import Iterable\n",
    "\n",
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "torch.manual_seed(1)\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    list(train_dataset), [20000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 69023\n"
     ]
    }
   ],
   "source": [
    "## Step 2: find unique tokens (words)\n",
    "import re \n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall(\n",
    "        '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()\n",
    "    )\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + \\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "\n",
    "token_counts = Counter()\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 3: encoding each uniqe token into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(\n",
    "    token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab = vocab(ordered_dict)\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7, 35, 457]\n"
     ]
    }
   ],
   "source": [
    "print([vocab[token] for token in ['this', 'is', 'an', 'example']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3-A: define the functions for transfoemation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 'pos' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3-B: wrap the encode and transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text),\n",
    "                                      dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list, batch_first=True)\n",
    "    return padded_text_list, label_list, lengths\n",
    "\n",
    "## Take a small batch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=4,\n",
    "                        shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   35,  1739,     7,   449,   721,     6,   301,     4,   787,     9,\n",
      "             4,    18,    44,     2,  1705,  2460,   186,    25,     7,    24,\n",
      "           100,  1874,  1739,    25,     7, 34415,  3568,  1103,  7517,   787,\n",
      "             5,     2,  4991, 12401,    36,     7,   148,   111,   939,     6,\n",
      "         11598,     2,   172,   135,    62,    25,  3199,  1602,     3,   928,\n",
      "          1500,     9,     6,  4601,     2,   155,    36,    14,   274,     4,\n",
      "         42945,     9,  4991,     3,    14, 10296,    34,  3568,     8,    51,\n",
      "           148,    30,     2,    58,    16,    11,  1893,   125,     6,   420,\n",
      "          1214,    27, 14542,   940,    11,     7,    29,   951,    18,    17,\n",
      "         15994,   459,    34,  2480, 15211,  3713,     2,   840,  3200,     9,\n",
      "          3568,    13,   107,     9,   175,    94,    25,    51, 10297,  1796,\n",
      "            27,   712,    16,     2,   220,    17,     4,    54,   722,   238,\n",
      "           395,     2,   787,    32,    27,  5236,     3,    32,    27,  7252,\n",
      "          5118,  2461,  6390,     4,  2873,  1495,    15,     2,  1054,  2874,\n",
      "           155,     3,  7015,     7,   409,     9,    41,   220,    17,    41,\n",
      "           390,     3,  3925,   807,    37,    74,  2858,    15, 10297,   115,\n",
      "            31,   189,  3506,   667,   163,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  216,   175,   724,     5,    11,    18,    10,   226,   110,    14,\n",
      "           182,    78,     8,    13,    24,   182,    78,     8,    13,   166,\n",
      "           182,    50,   150,    24,    85,     2,  4031,  5935,   107,    96,\n",
      "            28,  1867,   602,    19,    52,   162,    21,  1698,     8,     6,\n",
      "          1181,   367,     2,   351,    10,   140,   419,     4,   333,     5,\n",
      "          6022,  7136,  5055,  1209, 10892,    32,   219,     9,     2,   405,\n",
      "          1413,    13,  4031,    13,  1099,     7,    85,    19,     2,    20,\n",
      "          1018,     4,    85,   565,    34,    24,   807,    55,     5,    68,\n",
      "           658,    10,   507,     8,     4,   668,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   10,   121,    24,    28,    98,    74,   589,     9,   149,     2,\n",
      "          7372,  3030, 14543,  1012,   520,     2,   985,  2327,     5, 16847,\n",
      "          5479,    19,    25,    67,    76,  3478,    38,     2,  7372,     3,\n",
      "            25,    67,    76,  2951,    34,    35, 10893,   155,   449, 29495,\n",
      "         23725,    10,    67,     2,   554,    12, 14543,    67,    91,     4,\n",
      "            50,    20,    19,     8,    67,    24,  4228,     2,  2142,    37,\n",
      "            33,  3478,    87,     3,  2564,   160,   155,    11,   634,   126,\n",
      "            24,   158,    72,   286,    13,   373,     2,  4804,    19,     2,\n",
      "          7372,  6794,     6,    30,   128,    73,    48,    10,   886,     8,\n",
      "            13,    24,     4,    85,    20,    19,     8,    13,    35,   218,\n",
      "             3,   428,   710,     2,   107,   936,     7,    54,    72,   223,\n",
      "             3,    10,    96,   122,     2,   103,    54,    72,    82,     2,\n",
      "           658,   202,     2,   106,   293,   103,     7,  1193,     3,  3031,\n",
      "           708,  5760,     3,  2918,  3991,   706,  3327,   349,   148,   286,\n",
      "            13,   139,     6,     2,  1501,   750,    29,  1407,    62,    65,\n",
      "          2612,    71,    40,    14,     4,   547,     9,    62,     8,  7943,\n",
      "            71,    14,     2,  5687,     5,  4868,  3111,     6,   205,     2,\n",
      "            18,    55,  2075,     3,   403,    12,  3111,   231,    45,     5,\n",
      "           271,     3,    68,  1400,     7,  9774,   932,    10,   102,     2,\n",
      "            20,   143,    28,    76,    55,  3810,     9,  2723,     5,    12,\n",
      "            10,   379,     2,  7372,    15,     4,    50,   710,     8,    13,\n",
      "            24,   887,    32,    31,    19,     8,    13,   428],\n",
      "        [18923,     7,     4,  4753,  1669,    12,  3019,     6,     4, 13906,\n",
      "           502,    40,    25,    77,  1588,     9,   115,     6, 21713,     2,\n",
      "            90,   305,   237,     9,   502,    33,    77,   376,     4, 16848,\n",
      "           847,    62,    77,   131,     9,     2,  1580,   338,     5, 18923,\n",
      "            32,     2,  1980,    49,   157,   306, 21713,    46,   981,     6,\n",
      "         10298,     2, 18924,   125,     9,   502,     3,   453,     4,  1852,\n",
      "           630,   407,  3407,    34,   277,    29,   242,     2, 20200,     5,\n",
      "         18923,    77,    95,    41,  1833,     6,  2105,    56,     3,   495,\n",
      "           214,   528,     2,  3479,     2,   112,     7,   181,  1813,     3,\n",
      "           597,     5,     2,   156,   294,     4,   543,   173,     9,  1562,\n",
      "           289, 10038,     5,     2,    20,    26,   841,  1392,    62,   130,\n",
      "           111,    72,   832,    26,   181, 12402,    15,    69,   183,     6,\n",
      "            66,    55,   936,     5,     2,    63,     8,     7,    43,     4,\n",
      "            78, 23726, 15995,    13,    20,    17,   800,     5,   392,    59,\n",
      "          3992,     3,   371,   103,  2596,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
    "print(text_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([165,  86, 218, 145])\n"
     ]
    }
   ],
   "source": [
    "print(length_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 218])\n"
     ]
    }
   ],
   "source": [
    "print(text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                      shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                      shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                     shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7039, -0.8321, -0.4651],\n",
      "         [-0.3203,  2.2408,  0.5566],\n",
      "         [-0.4643,  0.3046,  0.7046],\n",
      "         [-0.7106, -0.2959,  0.8356]],\n",
      "\n",
      "        [[-0.4643,  0.3046,  0.7046],\n",
      "         [ 0.0946, -0.3531,  0.9124],\n",
      "         [-0.3203,  2.2408,  0.5566],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(\n",
    "    num_embeddings=10,\n",
    "    embedding_dim=3, \n",
    "    padding_idx=0)\n",
    "\n",
    "# a batch of 2 samples of 4 indices each\n",
    "text_encoded_input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 0]])\n",
    "print(embedding(text_encoded_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3183],\n",
       "        [ 0.1230],\n",
       "        [ 0.1772],\n",
       "        [-0.1052],\n",
       "        [-0.1259]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=2,\n",
    "                          batch_first=True)\n",
    "        # self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n",
    "        #                   batch_first=True)\n",
    "        # self.rnn = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "        #                    batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        out = hidden[-1, :, :] # we use the final hidden state\n",
    "                               # from the last hidden layer as\n",
    "                               # the input to the fully connected\n",
    "                               # layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "model = RNN(64, 32)\n",
    "print(model)\n",
    "model(torch.randn(5, 3, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN model for the sentiment analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(69025, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size,\n",
    "                 fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                      embed_dim,\n",
    "                                      padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text, lenghts):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lenghts.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "vocab_size =len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim,\n",
    "            rnn_hidden_size, fc_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8657,  0.2444, -0.6629,  ...,  0.0457,  0.1530, -0.4757],\n",
      "        [-0.1110,  0.2927, -0.1578,  ...,  0.9386, -0.1860, -0.6446],\n",
      "        ...,\n",
      "        [-0.2785,  0.8129,  1.2274,  ..., -0.7731, -0.0899,  0.5366],\n",
      "        [ 0.4883,  0.5243,  1.7338,  ..., -0.1695,  0.0799, -0.5368],\n",
      "        [ 0.4364, -0.4967,  0.7895,  ...,  0.9279,  0.4880, -0.4016]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0870, -0.0361,  0.0803,  ..., -0.1075,  0.0900,  0.0527],\n",
      "        [ 0.0688, -0.0678,  0.0287,  ..., -0.0374,  0.0662,  0.0892],\n",
      "        [ 0.0872,  0.0168, -0.0888,  ..., -0.0279,  0.0633, -0.0517],\n",
      "        ...,\n",
      "        [-0.0140, -0.0140, -0.0836,  ...,  0.0554,  0.0851,  0.1121],\n",
      "        [ 0.1217,  0.0716,  0.0637,  ...,  0.0082, -0.1137, -0.0793],\n",
      "        [-0.0300,  0.0681,  0.0968,  ...,  0.0732,  0.0752,  0.0205]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0969, -0.0251, -0.0180,  ..., -0.0654,  0.1081,  0.1212],\n",
      "        [ 0.0752,  0.0762,  0.0084,  ..., -0.0327,  0.0392, -0.0834],\n",
      "        [ 0.0090, -0.0085,  0.1209,  ...,  0.1198, -0.1113,  0.0229],\n",
      "        ...,\n",
      "        [ 0.1166,  0.0503, -0.0738,  ..., -0.0821, -0.0913, -0.0244],\n",
      "        [ 0.1164,  0.0621,  0.0518,  ...,  0.0364,  0.1167, -0.1020],\n",
      "        [-0.0526, -0.0970, -0.0048,  ..., -0.0301, -0.0305,  0.0053]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0143,  0.0725, -0.0991, -0.1067,  0.0701,  0.0647, -0.1142,  0.0792,\n",
      "         0.0300, -0.0588, -0.0690,  0.0725,  0.0407, -0.0939, -0.0092,  0.0915,\n",
      "        -0.1053,  0.0833,  0.0104, -0.0018, -0.0010,  0.0358, -0.1160, -0.1114,\n",
      "         0.0784,  0.0561, -0.0312, -0.1169,  0.0301, -0.1186,  0.0102, -0.0085,\n",
      "        -0.0630,  0.1228,  0.0619,  0.1022,  0.0566,  0.1200, -0.0172, -0.0833,\n",
      "        -0.0039, -0.0863, -0.0754, -0.0823, -0.0841, -0.0151,  0.0105, -0.0910,\n",
      "        -0.0639,  0.0907,  0.0922, -0.1212,  0.0233,  0.0483, -0.0254, -0.0251,\n",
      "        -0.0470, -0.0907,  0.0217,  0.0352, -0.0555, -0.0754,  0.0520,  0.0938,\n",
      "         0.0973, -0.0863,  0.0784, -0.0794,  0.0944,  0.0944, -0.0132,  0.1054,\n",
      "         0.0609,  0.0083, -0.1027,  0.0257,  0.1063,  0.0245, -0.0300,  0.0740,\n",
      "        -0.0675,  0.1053,  0.0490, -0.0189, -0.0629,  0.0857, -0.1177,  0.0742,\n",
      "        -0.1226,  0.1228, -0.1230, -0.0087, -0.0564, -0.1224,  0.0089, -0.0606,\n",
      "         0.0768, -0.0935,  0.0538, -0.1152,  0.1228, -0.0677,  0.0811,  0.0140,\n",
      "        -0.0738,  0.1072,  0.0075,  0.0869, -0.0868,  0.0117, -0.1020, -0.0175,\n",
      "         0.1243,  0.0539,  0.0123,  0.0837, -0.1127,  0.0192,  0.0139,  0.1180,\n",
      "        -0.0201, -0.0941, -0.0637,  0.0485, -0.0299,  0.0198, -0.0849,  0.1166,\n",
      "        -0.0929,  0.1188, -0.0067,  0.0714,  0.0980, -0.1050, -0.0957, -0.0417,\n",
      "         0.0245,  0.0769, -0.0034, -0.0214, -0.0819,  0.0284, -0.1012,  0.0531,\n",
      "        -0.0123,  0.0021,  0.0441,  0.0695, -0.0565,  0.0755,  0.0246,  0.0656,\n",
      "         0.0516,  0.0092,  0.0386, -0.0833,  0.0200,  0.0592, -0.1092,  0.0028,\n",
      "        -0.0554,  0.0022, -0.1210, -0.0262, -0.0262,  0.0788,  0.0297, -0.0204,\n",
      "         0.1047, -0.1063,  0.0255,  0.0763, -0.0272, -0.0453,  0.0976, -0.0266,\n",
      "        -0.0158,  0.0712, -0.1142,  0.0713,  0.0884, -0.0593, -0.0003, -0.0839,\n",
      "         0.0723,  0.0748, -0.0081,  0.1069,  0.0304, -0.0094,  0.0086, -0.0803,\n",
      "         0.0304,  0.1077, -0.0108, -0.0139, -0.1092,  0.0348,  0.1138, -0.1062,\n",
      "        -0.0121,  0.0380,  0.0049,  0.0683, -0.0569,  0.0171, -0.0522,  0.0237,\n",
      "        -0.0643,  0.0949,  0.0973, -0.0490, -0.1093, -0.0858,  0.0430, -0.0358,\n",
      "        -0.0092, -0.0350, -0.0086,  0.0414,  0.0468, -0.0071, -0.0033, -0.0331,\n",
      "         0.0052, -0.0920, -0.0142, -0.0717, -0.0561,  0.0119, -0.0454,  0.1171,\n",
      "        -0.0075,  0.0470,  0.0032,  0.0210,  0.0222, -0.0814, -0.0739,  0.0393,\n",
      "         0.0071, -0.0666,  0.1149, -0.0936, -0.0015, -0.0541, -0.0815,  0.0068,\n",
      "         0.0076, -0.0821, -0.0558, -0.0170, -0.0901, -0.0055,  0.0631,  0.0843],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0383,  0.0666, -0.0974, -0.0643, -0.0577,  0.1250, -0.1129,  0.0593,\n",
      "         0.0226,  0.0969,  0.0449,  0.0391,  0.0637, -0.0965, -0.0523,  0.1062,\n",
      "        -0.0527, -0.0113,  0.0935,  0.0698, -0.0407, -0.0081, -0.0938, -0.1038,\n",
      "         0.0723,  0.1116,  0.0940, -0.0804,  0.0794,  0.0784, -0.0732, -0.0895,\n",
      "         0.0795,  0.0646,  0.0473,  0.0239,  0.0176, -0.0257,  0.0384, -0.0616,\n",
      "        -0.1088, -0.0857, -0.1163,  0.0635,  0.0919, -0.0349,  0.0726,  0.0410,\n",
      "        -0.0191,  0.1174,  0.1217,  0.1102,  0.0639,  0.1069,  0.1087, -0.1089,\n",
      "        -0.0785, -0.0832,  0.0583,  0.1014,  0.0425,  0.1022, -0.0213,  0.0692,\n",
      "         0.0898, -0.0079, -0.1151, -0.0621, -0.0176,  0.0780,  0.0851, -0.0678,\n",
      "        -0.0844, -0.0792,  0.0466,  0.0426, -0.0383, -0.0751,  0.0561, -0.0743,\n",
      "         0.0004,  0.0127, -0.0828, -0.0023, -0.1070,  0.0102,  0.0853, -0.0477,\n",
      "        -0.0525,  0.1246, -0.0912,  0.0280, -0.0126,  0.0768,  0.0118,  0.1058,\n",
      "         0.0078, -0.1187,  0.0409, -0.0563, -0.0730, -0.0398, -0.0843, -0.0526,\n",
      "        -0.0622, -0.0185, -0.1129, -0.0320, -0.0740, -0.0843, -0.0321, -0.0918,\n",
      "        -0.0505,  0.1012,  0.0353,  0.0060,  0.0986, -0.0262,  0.0076, -0.0794,\n",
      "        -0.0327, -0.0595,  0.0381,  0.0490,  0.0321, -0.1239, -0.0274, -0.0667,\n",
      "        -0.0968,  0.0659, -0.1115,  0.0213,  0.0184, -0.1193,  0.0774,  0.1102,\n",
      "        -0.1194,  0.1158, -0.0359, -0.0266, -0.0826,  0.0497,  0.0255, -0.0419,\n",
      "        -0.0841,  0.1053,  0.0822,  0.1052, -0.0578,  0.0630,  0.0256, -0.0883,\n",
      "        -0.1205, -0.0650, -0.0223, -0.0082,  0.1034, -0.0349, -0.0660,  0.0291,\n",
      "         0.0395,  0.0906,  0.1171,  0.0752, -0.0159,  0.0609, -0.0915, -0.0430,\n",
      "         0.1055, -0.0679, -0.0844, -0.0193,  0.0103, -0.0039,  0.0196,  0.0754,\n",
      "         0.0092, -0.0528,  0.0497,  0.1237,  0.0242,  0.0455, -0.0837,  0.0845,\n",
      "        -0.0470, -0.0488,  0.0910, -0.0536,  0.1240, -0.1238, -0.1196,  0.0906,\n",
      "         0.0794, -0.0507,  0.0888, -0.0031,  0.1038, -0.0271,  0.1129, -0.0181,\n",
      "        -0.0119,  0.1241,  0.0794,  0.0936,  0.0114,  0.0075, -0.0135,  0.0585,\n",
      "        -0.0271,  0.1018, -0.1081,  0.0449, -0.1047, -0.0960,  0.0683, -0.1178,\n",
      "         0.1215, -0.1043,  0.0903, -0.0270,  0.0634,  0.1087,  0.0441,  0.0915,\n",
      "        -0.0549, -0.0336, -0.0676,  0.0007,  0.1072, -0.1187, -0.0662, -0.0246,\n",
      "        -0.0246,  0.0983, -0.0452,  0.0777,  0.1110,  0.0360, -0.0774,  0.0952,\n",
      "         0.0042,  0.0043,  0.0432,  0.1178, -0.0006,  0.0616,  0.0121,  0.0108,\n",
      "         0.1104, -0.0790, -0.0268,  0.0311,  0.1223, -0.1093,  0.0528, -0.0633],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0203, -0.0452,  0.0073,  ...,  0.0712, -0.0007,  0.0268],\n",
      "        [ 0.0429, -0.0907,  0.0553,  ...,  0.0821, -0.0273, -0.0776],\n",
      "        [-0.0916, -0.0778,  0.0532,  ...,  0.0024,  0.0588, -0.1141],\n",
      "        ...,\n",
      "        [ 0.0284, -0.1128,  0.0009,  ..., -0.0350, -0.0294,  0.1008],\n",
      "        [ 0.0652,  0.0743,  0.0330,  ...,  0.0914,  0.0191, -0.0704],\n",
      "        [ 0.0536,  0.0495, -0.1097,  ...,  0.0153, -0.1177,  0.0527]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0959, -0.0211,  0.1066, -0.0337, -0.0781, -0.0341,  0.0950, -0.0658,\n",
      "         0.0246,  0.0399,  0.0561, -0.0709, -0.0350, -0.0011,  0.0526, -0.0375,\n",
      "        -0.0784,  0.0189, -0.0690,  0.1017,  0.0137,  0.0456, -0.0245,  0.1212,\n",
      "        -0.1029,  0.0154,  0.1245,  0.0124, -0.0609, -0.0234, -0.0323,  0.0307,\n",
      "        -0.0275,  0.0277,  0.0120,  0.0101, -0.0496,  0.0527,  0.0778, -0.0613,\n",
      "         0.1202,  0.0117, -0.1208,  0.0007,  0.0832, -0.1063, -0.0590,  0.0363,\n",
      "         0.0393,  0.0205,  0.1133,  0.1041, -0.0992,  0.0571,  0.1084, -0.1167,\n",
      "        -0.0020, -0.0894, -0.0652, -0.0494,  0.1058,  0.0715,  0.0580, -0.0100],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0336, -0.0124, -0.0741,  0.0318,  0.0555, -0.0961,  0.0075,  0.1045,\n",
      "         -0.0178, -0.0328, -0.0071, -0.0248,  0.0519, -0.0984, -0.0210,  0.0414,\n",
      "         -0.0298,  0.0578, -0.0061, -0.1001, -0.0079,  0.1030, -0.0398,  0.1099,\n",
      "         -0.0772, -0.1055,  0.0096,  0.1175, -0.0512, -0.0593,  0.1086, -0.0617,\n",
      "         -0.1193,  0.0326,  0.0664,  0.1073,  0.0757, -0.0511, -0.0528, -0.0551,\n",
      "          0.0896, -0.1017,  0.0682,  0.0954, -0.0874, -0.0152,  0.0764,  0.0490,\n",
      "         -0.0365, -0.0509, -0.0377, -0.1033,  0.0175,  0.0393, -0.0892,  0.0646,\n",
      "         -0.0659, -0.0743, -0.0773,  0.1072, -0.0469, -0.0109, -0.0027, -0.0970]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0285], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "mod = model.parameters()\n",
    "\n",
    "for p in mod:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lenghts in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lenghts)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (\n",
    "            (pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "        \n",
    "    return total_acc/len(dataloader.dataset), \\\n",
    "           total_loss/len(dataloader.dataset)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += (\n",
    "                (pred >= 0.5).float() == label_batch\n",
    "            ).float().sum().item()\n",
    "            total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc / len(dataloader.dataset), \\\n",
    "           total_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to create a loss function and optimizer (Adam optimizer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w chuj dÅ‚ugo sie trenuje\n",
    "\n",
    "# loss_fn = nn.BCELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # train\n",
    "\n",
    "# num_epochs = 10\n",
    "# torch.manual_seed(1)\n",
    "# for epoch in range(num_epochs):\n",
    "#     acc_train, loss_train = train(train_dl)\n",
    "#     acc_valid, loss_valid = evaluate(valid_dl)\n",
    "#     print(f'Epoch {epoch} accuracy: {acc_train:.4f}'\n",
    "#           f'val_accuracy: {acc_valid:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error \n",
    "\n",
    "# acc_test, _ = evaluate(test_dl)\n",
    "# print(f'test_accuracy: {acc_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(69025, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim,\n",
    "                 rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n",
    "        self.relu= nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text, lenghts):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        _, (hidden, cell) = self.rnn(out)\n",
    "        out = torch.cat((hidden[-2, :, :],\n",
    "                         hidden[-1, :, :]), dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, \n",
    "            rnn_hidden_size, fc_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project two â€“ character-level language modeling in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (9755894.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [33]\u001b[1;36m\u001b[0m\n\u001b[1;33m    curl -O https://www.gutenberg.org/files/1268/1268-0.txt\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# curl -O https://www.gutenberg.org/files/1268/1268-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Lenght: 1130711\n",
      "Uniqe Characters: 85\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "## Reading and processing text\n",
    "\n",
    "with open('1268-0.txt', 'r', encoding='utf8') as fp:\n",
    "    text=fp.read()\n",
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_indx = text.find('End of the Project Gutenberg')\n",
    "text = text[start_indx:end_indx]\n",
    "char_set = set(text)\n",
    "print('Total Lenght:', len(text))\n",
    "print('Uniqe Characters:', len(char_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building the dictionary to map characters to integers, and reverse mapping via indexing a NumPy\n",
    "# array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape: (1130711,)\n"
     ]
    }
   ],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i, ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "text_encoded = np.array([char2int[ch] for ch in text],\n",
    "                      dtype=np.int32)\n",
    "print('Text encoded shape:', text_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE MYSTERIOUS  == Encoding ==> [48 36 33  1 41 53 47 48 33 46 37 43 49 47  1]\n",
      "[37 47 40 29 42 32] == Reverse ==> ISLAND\n"
     ]
    }
   ],
   "source": [
    "print(text[:15], '== Encoding ==>', text_encoded[:15])\n",
    "print(text_encoded[15:21], '== Reverse ==>',\n",
    "      ''.join(char_array[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 -> T\n",
      "36 -> H\n",
      "33 -> E\n",
      "1 ->  \n",
      "41 -> M\n"
     ]
    }
   ],
   "source": [
    "for ex in text_encoded[:5]:\n",
    "    print('{} -> {}'.format(ex, char_array[ex]))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input (x):  'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'\n",
      "Target (y):  'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "\n",
      " Input (x):  'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "Target (y):  'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERIO'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PipBoy3000\\AppData\\Local\\Temp\\ipykernel_18796\\4051511425.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "text_chunks = [text_encoded[i:i + chunk_size]\n",
    "               for i in range(len(text_encoded)-chunk_size)]\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "    \n",
    "    \n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))\n",
    "\n",
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    print(' Input (x): ',\n",
    "          repr(''.join(char_array[seq])))\n",
    "    print('Target (y): ',\n",
    "          repr(''.join(char_array[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size,\n",
    "                    shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Mosule):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "    \n",
    "    def init_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
