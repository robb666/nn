{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: Parameter containing:\n",
      "tensor([[ 0.3643, -0.3121, -0.1371,  0.3319, -0.6657],\n",
      "        [ 0.4241, -0.1455,  0.3597,  0.0983, -0.0866]], requires_grad=True)\n",
      "\n",
      "W_xh shape: torch.Size([2, 5])\n",
      "W_hh shape: torch.Size([2, 2])\n",
      "b_xh shape: torch.Size([2])\n",
      "b_hh shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "rnn_layer = nn.RNN(input_size=5, hidden_size=2,\n",
    "                   num_layers=1, batch_first=True)\n",
    "w_xh = rnn_layer.weight_ih_l0\n",
    "w_hh = rnn_layer.weight_hh_l0\n",
    "b_xh = rnn_layer.bias_ih_l0\n",
    "b_hh = rnn_layer.bias_hh_l0\n",
    "\n",
    "print('W_xh shape:', w_xh)\n",
    "print()\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_hh shape:', w_hh.shape)\n",
    "print('b_xh shape:', b_xh.shape)\n",
    "print('b_hh shape:', b_hh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "    Input           : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden           : [[-0.3161478   0.64722455]]\n",
      "    Output (manual) : [[-0.21046415  0.56788784]]\n",
      "    RNN output      : [[-0.3519801   0.52525216]]\n",
      "\n",
      "Time step 1 =>\n",
      "    Input           : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden           : [[-0.73478645  1.2972739 ]]\n",
      "    Output (manual) : [[-0.5741978  0.7945334]]\n",
      "    RNN output      : [[-0.68424344  0.76074266]]\n",
      "\n",
      "Time step 2 =>\n",
      "    Input           : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden           : [[-1.153425   1.9473232]]\n",
      "    Output (manual) : [[-0.8130059   0.91817397]]\n",
      "    RNN output      : [[-0.8649416   0.90466356]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
    "\n",
    "## output of the simple RNN:\n",
    "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n",
    "\n",
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1, 5))\n",
    "    print(f'Time step {t} =>')\n",
    "    print('    Input           :', xt.numpy())\n",
    "    \n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_hh\n",
    "    print('   Hidden           :', ht.detach().numpy())\n",
    "    if t > 0:\n",
    "         prev_h = out_man[t-1]\n",
    "    else:\n",
    "         prev_h = torch.zeros((ht.shape))\n",
    "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) \\\n",
    "            + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('    Output (manual) :', ot.detach().numpy())\n",
    "    print('    RNN output      :', output[:, t].detach().numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 4]' is invalid for input of size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 4]' is invalid for input of size 5"
     ]
    }
   ],
   "source": [
    "torch.reshape(x_seq[t], (1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Project one â€“ predicting the sentiment of IMDb movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "from collections.abc import Iterable\n",
    "\n",
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "torch.manual_seed(1)\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    list(train_dataset), [20000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 69344\n"
     ]
    }
   ],
   "source": [
    "## Step 2: find unique tokens (words)\n",
    "import re \n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall(\n",
    "        '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()\n",
    "    )\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + \\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "\n",
    "token_counts = Counter()\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 3: encoding each uniqe token into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(\n",
    "    token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab = vocab(ordered_dict)\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7, 35, 458]\n"
     ]
    }
   ],
   "source": [
    "print([vocab[token] for token in ['this', 'is', 'an', 'example']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3-A: define the functions for transfoemation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 'pos' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3-B: wrap the encode and transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text),\n",
    "                                      dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list, batch_first=True)\n",
    "    return padded_text_list, label_list, lengths\n",
    "\n",
    "## Take a small batch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=4,\n",
    "                        shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 163,   10,  880,  ...,    6,    2,  872],\n",
      "        [  40,   45,   77,  ...,    0,    0,    0],\n",
      "        [4894,   10,  243,  ...,    0,    0,    0],\n",
      "        [ 322,   85,  511,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
    "print(text_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([925, 249,  85, 125])\n"
     ]
    }
   ],
   "source": [
    "print(length_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 925])\n"
     ]
    }
   ],
   "source": [
    "print(text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                      shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                      shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                     shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7039, -0.8321, -0.4651],\n",
      "         [-0.3203,  2.2408,  0.5566],\n",
      "         [-0.4643,  0.3046,  0.7046],\n",
      "         [-0.7106, -0.2959,  0.8356]],\n",
      "\n",
      "        [[-0.4643,  0.3046,  0.7046],\n",
      "         [ 0.0946, -0.3531,  0.9124],\n",
      "         [-0.3203,  2.2408,  0.5566],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(\n",
    "    num_embeddings=10,\n",
    "    embedding_dim=3, \n",
    "    padding_idx=0)\n",
    "\n",
    "# a batch of 2 samples of 4 indices each\n",
    "text_encoded_input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 0]])\n",
    "print(embedding(text_encoded_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3183],\n",
       "        [ 0.1230],\n",
       "        [ 0.1772],\n",
       "        [-0.1052],\n",
       "        [-0.1259]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=2,\n",
    "                          batch_first=True)\n",
    "        # self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n",
    "        #                   batch_first=True)\n",
    "        # self.rnn = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "        #                    batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        out = hidden[-1, :, :] # we use the final hidden state\n",
    "                               # from the last hidden layer as\n",
    "                               # the input to the fully connected\n",
    "                               # layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "model = RNN(64, 32)\n",
    "print(model)\n",
    "model(torch.randn(5, 3, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN model for the sentiment analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(69346, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size,\n",
    "                 fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                      embed_dim,\n",
    "                                      padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text, lenghts):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lenghts.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "vocab_size =len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim,\n",
    "            rnn_hidden_size, fc_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8657,  0.2444, -0.6629,  ...,  0.0457,  0.1530, -0.4757],\n",
      "        [-0.1110,  0.2927, -0.1578,  ...,  0.9386, -0.1860, -0.6446],\n",
      "        ...,\n",
      "        [-0.1404, -0.1202,  0.8836,  ..., -0.0382, -0.0254,  0.5872],\n",
      "        [ 1.4415, -1.3369, -0.1999,  ...,  1.2954, -1.2241, -0.9001],\n",
      "        [ 0.3040, -1.1861,  0.5460,  ...,  1.1956,  0.2319,  0.6994]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0742,  0.0847,  0.0768,  ..., -0.0319, -0.0131, -0.0751],\n",
      "        [-0.0221,  0.0828, -0.1203,  ...,  0.0752,  0.0840, -0.0027],\n",
      "        [ 0.1068, -0.0608, -0.0829,  ..., -0.0267,  0.0347, -0.0030],\n",
      "        ...,\n",
      "        [ 0.1153, -0.1099,  0.0404,  ...,  0.0444, -0.0708,  0.0810],\n",
      "        [ 0.0772, -0.0308,  0.0414,  ..., -0.1104,  0.0609,  0.0099],\n",
      "        [-0.0186, -0.0078, -0.1045,  ..., -0.1064,  0.0693, -0.0206]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1175, -0.0234,  0.0656,  ..., -0.0991, -0.0315, -0.1103],\n",
      "        [-0.1177, -0.0093,  0.1229,  ..., -0.0319, -0.0539, -0.0604],\n",
      "        [-0.0065, -0.0846,  0.0809,  ...,  0.0042,  0.0654,  0.1070],\n",
      "        ...,\n",
      "        [ 0.0856,  0.0302, -0.0119,  ...,  0.0054, -0.0273,  0.0811],\n",
      "        [-0.0222,  0.1156, -0.0912,  ...,  0.0031,  0.0289,  0.0027],\n",
      "        [ 0.0669,  0.0308, -0.0113,  ..., -0.1000, -0.0444, -0.0492]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0754, -0.1185,  0.0350, -0.1098,  0.0664, -0.1047, -0.0095, -0.0058,\n",
      "        -0.0735,  0.0590, -0.0772,  0.0528, -0.0583,  0.0255, -0.0828,  0.1144,\n",
      "        -0.0028,  0.1245,  0.0638,  0.0858, -0.0298, -0.0694,  0.0987,  0.0918,\n",
      "         0.0086,  0.0969, -0.0113,  0.0759, -0.0876, -0.0947, -0.1000, -0.0157,\n",
      "         0.0196,  0.0404, -0.1090, -0.0720, -0.0452,  0.0967,  0.0714, -0.1245,\n",
      "        -0.0477, -0.0610,  0.0805,  0.0028, -0.0860, -0.0152, -0.1040, -0.1001,\n",
      "        -0.0199, -0.0602, -0.0239,  0.0238,  0.0279, -0.0784,  0.1235,  0.0199,\n",
      "        -0.0231, -0.1075, -0.1228,  0.1076,  0.1210,  0.0816,  0.0417, -0.1018,\n",
      "         0.0649, -0.0562, -0.0745,  0.0856, -0.0554, -0.0322,  0.1175, -0.0749,\n",
      "         0.0499, -0.0960, -0.1030, -0.1224, -0.0117,  0.0184, -0.1056, -0.0930,\n",
      "        -0.0921,  0.0855, -0.0268,  0.0068,  0.0094, -0.1121,  0.0885, -0.0898,\n",
      "        -0.0893, -0.0082, -0.0438, -0.0724,  0.1190, -0.0863, -0.0570, -0.0970,\n",
      "        -0.0020,  0.0198, -0.1055,  0.1121, -0.0378, -0.0412,  0.0930,  0.0811,\n",
      "        -0.0693,  0.0243, -0.1058,  0.0839,  0.0415,  0.0967,  0.0136, -0.0127,\n",
      "        -0.0727, -0.1078, -0.0532, -0.0550, -0.0297, -0.0218,  0.1115, -0.0749,\n",
      "         0.0620, -0.0367,  0.0694,  0.1096,  0.1232,  0.0478, -0.0072, -0.1164,\n",
      "        -0.0061, -0.0755,  0.0085, -0.0867, -0.0636, -0.1032,  0.0823,  0.0802,\n",
      "        -0.1150,  0.0447,  0.0177,  0.0210, -0.0901, -0.1027, -0.0938,  0.0044,\n",
      "         0.0107, -0.0456,  0.0213,  0.0956, -0.0363,  0.0222, -0.0737, -0.1111,\n",
      "        -0.0933, -0.0499, -0.0281, -0.0071, -0.0482,  0.0124,  0.1190, -0.0063,\n",
      "        -0.0984,  0.1069,  0.1210,  0.0459,  0.0317,  0.0335, -0.0259,  0.0393,\n",
      "        -0.0308, -0.0895, -0.1105,  0.0040, -0.1128, -0.0414, -0.0737,  0.0239,\n",
      "        -0.0544,  0.0930,  0.0159, -0.0978, -0.0071,  0.0156, -0.0713, -0.0849,\n",
      "         0.0668,  0.0383, -0.0520, -0.0120, -0.0220, -0.1190,  0.0574,  0.0422,\n",
      "         0.0232,  0.0828,  0.0052, -0.0769, -0.0471, -0.0033, -0.0750,  0.1131,\n",
      "         0.1215,  0.1212, -0.0837,  0.0484,  0.1248,  0.0877, -0.0708, -0.0901,\n",
      "        -0.0424, -0.0114, -0.0694, -0.1047,  0.0805, -0.0173,  0.0445,  0.0903,\n",
      "        -0.0145, -0.0820, -0.0898,  0.0225, -0.0213, -0.0755, -0.0906, -0.0063,\n",
      "        -0.0605,  0.0738,  0.0336, -0.0443,  0.0492,  0.1085,  0.0154, -0.0227,\n",
      "        -0.0920,  0.0724,  0.0212,  0.0718,  0.0172, -0.0947,  0.0554, -0.0314,\n",
      "         0.0594, -0.0779,  0.0866,  0.0067, -0.1165,  0.0301,  0.0577, -0.0094,\n",
      "         0.0202,  0.0818, -0.0716,  0.0270,  0.0889, -0.1106,  0.0258,  0.0163],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0487, -0.0734,  0.0941,  0.0513, -0.0522, -0.1189,  0.0442,  0.0652,\n",
      "        -0.0903,  0.0035,  0.0829,  0.0794, -0.0958, -0.0903, -0.0200,  0.0667,\n",
      "         0.0789,  0.0522,  0.1185,  0.0390,  0.0700,  0.0139,  0.0065, -0.0195,\n",
      "         0.0940,  0.1095, -0.0381,  0.0184,  0.0947, -0.0419,  0.1026, -0.1179,\n",
      "        -0.1011, -0.0541,  0.0727,  0.0763, -0.0707, -0.0457, -0.1108, -0.1193,\n",
      "        -0.0365,  0.1030, -0.0483,  0.0632, -0.0256,  0.0529, -0.0756,  0.1119,\n",
      "         0.1151,  0.1178,  0.0664,  0.0500,  0.0430, -0.0106,  0.0538,  0.1011,\n",
      "        -0.0943, -0.0414,  0.0909,  0.0088, -0.1221,  0.0474, -0.0408, -0.0230,\n",
      "        -0.0549, -0.0379,  0.0868, -0.0386,  0.0659,  0.0668,  0.0098, -0.0786,\n",
      "         0.0741,  0.0378,  0.0649,  0.0703,  0.0003,  0.0140, -0.0222,  0.1039,\n",
      "        -0.0312,  0.0964,  0.0728,  0.0128, -0.0800,  0.0944,  0.0773, -0.0725,\n",
      "        -0.1049,  0.0102,  0.0402,  0.0434,  0.0441, -0.1240,  0.0113,  0.1207,\n",
      "         0.0400, -0.1225,  0.0700,  0.0250, -0.0790, -0.0129, -0.0518, -0.0147,\n",
      "        -0.0502, -0.0214,  0.0812, -0.0661, -0.0809, -0.0766, -0.0536,  0.0677,\n",
      "         0.0965,  0.0162,  0.0232,  0.0294,  0.1179,  0.0074, -0.0061, -0.0049,\n",
      "        -0.0743,  0.0495, -0.1018,  0.1127, -0.0162, -0.0478,  0.0697,  0.1165,\n",
      "        -0.0607, -0.0393, -0.1241,  0.0354, -0.1107, -0.1232, -0.0069, -0.1133,\n",
      "         0.1179,  0.0438,  0.0318,  0.1155, -0.1169,  0.1209,  0.1126, -0.0159,\n",
      "        -0.0038, -0.1069,  0.0121, -0.1040, -0.0792,  0.0462, -0.0881, -0.0785,\n",
      "         0.0628, -0.1197, -0.0145,  0.0282, -0.0598, -0.0716, -0.1249,  0.0838,\n",
      "         0.0957,  0.0050,  0.1087,  0.0808,  0.1176, -0.0050,  0.1182,  0.0762,\n",
      "         0.0640, -0.1107,  0.1208, -0.1150,  0.1056, -0.0380, -0.1104,  0.0037,\n",
      "         0.0975,  0.0983, -0.0132, -0.0616,  0.0102, -0.0990, -0.0678, -0.0642,\n",
      "        -0.0922, -0.0574,  0.1029,  0.0812, -0.0602,  0.0650,  0.0783, -0.0116,\n",
      "         0.1110,  0.1131,  0.0044, -0.0271,  0.0239, -0.0280, -0.0472,  0.0026,\n",
      "         0.0459, -0.0213, -0.0093, -0.0548, -0.0197, -0.0928, -0.0397, -0.0639,\n",
      "         0.0287, -0.0242,  0.0275,  0.1072, -0.0234,  0.0697, -0.0601, -0.0910,\n",
      "         0.0147, -0.0862, -0.0013,  0.0569, -0.0721, -0.0277,  0.0881, -0.0166,\n",
      "        -0.0679,  0.0607,  0.0866, -0.0912, -0.0095,  0.1141,  0.1107,  0.0172,\n",
      "         0.0355, -0.0358,  0.0980,  0.0093, -0.0598, -0.0326, -0.0474,  0.0843,\n",
      "        -0.0565, -0.0161, -0.1145,  0.0325,  0.0341,  0.0193, -0.1238,  0.0934,\n",
      "        -0.0685, -0.1142, -0.0664, -0.0828,  0.0387, -0.0968, -0.0925,  0.0254],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0543,  0.0982,  0.0548,  ...,  0.0241, -0.0099, -0.0604],\n",
      "        [-0.0811, -0.0631, -0.0568,  ..., -0.0598, -0.0237, -0.0362],\n",
      "        [-0.0946,  0.0277,  0.0476,  ..., -0.1225, -0.0711, -0.0401],\n",
      "        ...,\n",
      "        [-0.0488,  0.0195, -0.0278,  ...,  0.0544, -0.1095, -0.1073],\n",
      "        [ 0.0481, -0.1045,  0.0606,  ..., -0.0808,  0.0234, -0.1127],\n",
      "        [ 0.0449,  0.0288, -0.0779,  ...,  0.1043, -0.0050,  0.1017]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0885,  0.1232, -0.1120,  ...,  0.0599,  0.1203,  0.0817],\n",
      "        [-0.0204,  0.0806, -0.0034,  ..., -0.0269, -0.1084, -0.0428],\n",
      "        [-0.0104,  0.0959,  0.0678,  ...,  0.0803,  0.1086, -0.0919],\n",
      "        ...,\n",
      "        [-0.0864,  0.0134,  0.1204,  ..., -0.0233, -0.0528, -0.0699],\n",
      "        [-0.0785,  0.1216, -0.1205,  ..., -0.0104, -0.0980, -0.0328],\n",
      "        [-0.0413,  0.0932, -0.0730,  ..., -0.0274,  0.1142,  0.1070]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0777, -0.0567, -0.0675,  0.0608, -0.0564, -0.1146, -0.0805, -0.0316,\n",
      "        -0.1178,  0.0239, -0.0334, -0.0510, -0.0697,  0.0811,  0.0810,  0.0979,\n",
      "        -0.0525,  0.1227, -0.0430, -0.0530,  0.1171,  0.0833,  0.0703, -0.1031,\n",
      "        -0.0580,  0.0438, -0.0323, -0.0893, -0.0677, -0.0299,  0.1024,  0.1040,\n",
      "        -0.0675,  0.0092, -0.0338,  0.0799,  0.1095,  0.0511, -0.1183,  0.0391,\n",
      "        -0.1065,  0.0493,  0.0467,  0.0185, -0.1104,  0.0630, -0.0873,  0.0901,\n",
      "         0.0514, -0.0537,  0.0040,  0.0534, -0.0758,  0.0377, -0.0722, -0.0081,\n",
      "         0.0321,  0.0782, -0.0267, -0.0086, -0.0311, -0.0601, -0.0930,  0.0712,\n",
      "         0.1152,  0.0122, -0.1196,  0.0713, -0.1192,  0.0196,  0.0125, -0.0255,\n",
      "         0.0790, -0.0747, -0.0019, -0.1069,  0.1227,  0.0761, -0.1028, -0.1198,\n",
      "         0.0027, -0.0108, -0.0489,  0.1230,  0.0366, -0.0461,  0.0199, -0.0934,\n",
      "         0.0149,  0.0442,  0.0649, -0.0449, -0.1221,  0.0784,  0.0494,  0.0567,\n",
      "         0.0601,  0.0897,  0.0354,  0.0801,  0.0215, -0.0685,  0.1187, -0.0899,\n",
      "        -0.1150,  0.0401, -0.1102,  0.0816, -0.0960,  0.0806, -0.0965,  0.1141,\n",
      "         0.0946, -0.0453, -0.0315,  0.0355,  0.0031, -0.0701, -0.0880, -0.0960,\n",
      "         0.1026,  0.0771,  0.0855, -0.1047, -0.0724, -0.0161,  0.0643, -0.1129,\n",
      "        -0.0038,  0.0656, -0.0709, -0.1150, -0.0807,  0.0391, -0.0408,  0.1096,\n",
      "         0.0177, -0.0399, -0.1248,  0.0420,  0.0312, -0.0366,  0.0499,  0.0607,\n",
      "         0.0070, -0.0367,  0.0044, -0.0785, -0.0532, -0.1156,  0.0418,  0.0227,\n",
      "        -0.0781,  0.1081,  0.0528,  0.1149,  0.0203, -0.0618, -0.1063, -0.0161,\n",
      "        -0.0765, -0.0590, -0.0722,  0.0317, -0.0482,  0.0795, -0.0866,  0.0005,\n",
      "         0.0484, -0.0182, -0.0027, -0.0332,  0.1157,  0.0082,  0.0704,  0.0857,\n",
      "         0.1005, -0.1142,  0.0106,  0.0546, -0.0582, -0.1049, -0.0216,  0.0481,\n",
      "        -0.0781, -0.0389,  0.0889, -0.0826, -0.0367,  0.0013,  0.0941,  0.0409,\n",
      "        -0.0400, -0.0244,  0.0609,  0.0442,  0.1023,  0.1021, -0.0868,  0.0726,\n",
      "         0.0123,  0.0670,  0.0602,  0.0307,  0.0332, -0.0595, -0.0605, -0.0283,\n",
      "        -0.0585,  0.0302, -0.0318,  0.0699,  0.0551,  0.0252, -0.1230,  0.0520,\n",
      "        -0.0902,  0.0733,  0.0673,  0.0047, -0.0294,  0.0125,  0.0061, -0.1044,\n",
      "         0.1065,  0.0174, -0.0806,  0.0994,  0.0722, -0.0622,  0.0456,  0.0672,\n",
      "        -0.0944,  0.0157,  0.1204, -0.0131, -0.0801, -0.0805,  0.1139,  0.0453,\n",
      "        -0.0665, -0.0470,  0.0330,  0.0953, -0.0583, -0.1214, -0.1249, -0.1021,\n",
      "        -0.0934, -0.0028, -0.1002, -0.1151,  0.0885,  0.1115, -0.0109,  0.0344],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0579, -0.0305,  0.1239, -0.0414,  0.1163,  0.0621, -0.0400,  0.0822,\n",
      "        -0.0378,  0.0778, -0.0002, -0.0682,  0.0210, -0.0663,  0.0496,  0.1232,\n",
      "        -0.0334, -0.0100, -0.0838, -0.0959, -0.0599,  0.0123,  0.0937, -0.0893,\n",
      "        -0.1246, -0.0384,  0.0831,  0.0027,  0.0021,  0.0752, -0.0166,  0.0138,\n",
      "        -0.0172,  0.0142, -0.0227, -0.0927, -0.0293, -0.0674,  0.0450, -0.0492,\n",
      "        -0.0558, -0.0089, -0.0822, -0.0275,  0.0917,  0.0313, -0.0531, -0.0526,\n",
      "        -0.1009,  0.0723, -0.1062, -0.0013, -0.0828,  0.0054, -0.0439,  0.0997,\n",
      "        -0.0071,  0.0016,  0.0045, -0.0141,  0.0777, -0.0210, -0.0776,  0.1084,\n",
      "        -0.0449,  0.0975, -0.0674,  0.0054, -0.0596,  0.0454,  0.0051, -0.0911,\n",
      "        -0.0946, -0.0252, -0.0079,  0.0962,  0.1208,  0.0854, -0.0152, -0.0061,\n",
      "        -0.1014,  0.0648,  0.0689, -0.0678, -0.0318, -0.0446, -0.0162,  0.0240,\n",
      "        -0.0050,  0.0996, -0.0369, -0.0353, -0.0962, -0.0660, -0.0806,  0.0738,\n",
      "         0.0529, -0.0003, -0.0121,  0.1216, -0.0888, -0.0920, -0.0492,  0.0985,\n",
      "        -0.0999,  0.0603,  0.0948, -0.0989, -0.0407, -0.0739, -0.1186,  0.0215,\n",
      "         0.0551, -0.0303, -0.0621, -0.0658, -0.0418, -0.0178, -0.0226,  0.0911,\n",
      "         0.0778, -0.0461, -0.0876, -0.1214, -0.0002, -0.0879,  0.0999, -0.1207,\n",
      "        -0.1066,  0.0464,  0.0897,  0.0267,  0.0115,  0.1071,  0.0910,  0.0932,\n",
      "         0.1084,  0.1007, -0.0161,  0.0327,  0.0438, -0.1096,  0.0227,  0.0968,\n",
      "         0.0484,  0.0411,  0.0375,  0.0394,  0.0012, -0.1009, -0.0515,  0.1041,\n",
      "         0.0369,  0.0183,  0.0390, -0.1145, -0.0430,  0.0960,  0.0294, -0.0738,\n",
      "         0.0675, -0.0958,  0.0998, -0.0052,  0.0839,  0.0155, -0.1197,  0.0296,\n",
      "        -0.0284, -0.0930,  0.0008, -0.1198,  0.0004,  0.0061, -0.0355,  0.0950,\n",
      "         0.1208,  0.0444, -0.1143, -0.0298, -0.0138, -0.0566, -0.0397, -0.0425,\n",
      "         0.0535,  0.1095, -0.0668, -0.1132, -0.0949,  0.0270, -0.0810,  0.0180,\n",
      "        -0.0027, -0.1110,  0.0622,  0.1007, -0.0669,  0.0616, -0.0464,  0.0125,\n",
      "         0.0754, -0.1136, -0.0394, -0.0556,  0.0356, -0.1102,  0.1086, -0.0055,\n",
      "        -0.0625,  0.0637, -0.0650,  0.0605,  0.0602, -0.0668, -0.0939,  0.0310,\n",
      "        -0.0685,  0.0633, -0.0357, -0.0447,  0.0911,  0.0220,  0.0253, -0.0076,\n",
      "        -0.0322,  0.0116, -0.1070, -0.0833, -0.0773, -0.0924, -0.0404,  0.1000,\n",
      "         0.0615, -0.0514,  0.0927,  0.0126, -0.1048,  0.1140, -0.0097,  0.1114,\n",
      "        -0.0855, -0.0276, -0.0935, -0.1004, -0.1062,  0.0715,  0.0484,  0.0880,\n",
      "         0.0312,  0.0529, -0.0864,  0.0023, -0.0616,  0.0763,  0.0500,  0.0040],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0496,  0.0165,  0.0370,  ..., -0.0663,  0.0566, -0.0725],\n",
      "        [ 0.0487,  0.0876, -0.0101,  ..., -0.0653,  0.0452, -0.0719],\n",
      "        [-0.0409, -0.0833,  0.0621,  ...,  0.0256,  0.0003,  0.0057],\n",
      "        ...,\n",
      "        [ 0.0127,  0.0539, -0.0698,  ..., -0.0720, -0.0219, -0.0236],\n",
      "        [ 0.0771,  0.0030,  0.0190,  ..., -0.0454,  0.0722,  0.0469],\n",
      "        [-0.0618,  0.0189, -0.0176,  ...,  0.0487,  0.0483, -0.0693]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0210,  0.0115, -0.0843, -0.0765, -0.0727, -0.0239,  0.0747, -0.0352,\n",
      "        -0.0213,  0.0611, -0.0396,  0.0480, -0.0704, -0.0787,  0.0263, -0.0857,\n",
      "        -0.0095,  0.0755,  0.0301,  0.0586,  0.0868, -0.0442,  0.0184, -0.0021,\n",
      "        -0.0088,  0.0428,  0.0786,  0.0002,  0.0740, -0.0584,  0.0827, -0.0253,\n",
      "         0.0067, -0.0677,  0.0661,  0.0439, -0.0288,  0.0329,  0.0219, -0.0258,\n",
      "         0.0504, -0.0407, -0.0790, -0.0680, -0.0506, -0.0022, -0.0085, -0.0279,\n",
      "         0.0840, -0.0437, -0.0167, -0.0732,  0.0205,  0.0443,  0.0712, -0.0411,\n",
      "         0.0710,  0.0792, -0.0269, -0.0198,  0.0370, -0.0423,  0.0494,  0.0708],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0326,  0.0013, -0.0152,  0.0757,  0.0814,  0.1027,  0.0780, -0.0072,\n",
      "          0.0231,  0.0860, -0.0103,  0.1020,  0.1112, -0.0451, -0.1113, -0.0077,\n",
      "         -0.0971,  0.0765, -0.0809, -0.0692, -0.0858,  0.0252, -0.0346, -0.1147,\n",
      "          0.0166, -0.1153, -0.1206, -0.0228, -0.0530, -0.0889, -0.0261, -0.0883,\n",
      "         -0.0291, -0.0003,  0.0578, -0.1002, -0.1070,  0.0678,  0.0476,  0.0885,\n",
      "         -0.0509,  0.1092,  0.1189, -0.0551,  0.0353, -0.0291, -0.0574,  0.0574,\n",
      "         -0.0384, -0.0615, -0.1069,  0.0400,  0.0981, -0.0583,  0.0852, -0.0557,\n",
      "          0.0046,  0.0243,  0.0422,  0.0722, -0.1142,  0.0029, -0.0819, -0.0717]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0064], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "mod = model.parameters()\n",
    "\n",
    "for p in mod:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lenghts in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lenghts)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (\n",
    "            (pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "        \n",
    "    return total_acc/len(dataloader.dataset), \\\n",
    "           total_loss/len(dataloader.dataset)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += (\n",
    "                (pred >= 0.5).float() == label_batch\n",
    "            ).float().sum().item()\n",
    "            total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc / len(dataloader.dataset), \\\n",
    "           total_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to create a loss function and optimizer (Adam optimizer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.6010val_accuracy: 0.6608\n",
      "Epoch 1 accuracy: 0.7359val_accuracy: 0.7332\n",
      "Epoch 2 accuracy: 0.7707val_accuracy: 0.7946\n",
      "Epoch 3 accuracy: 0.8387val_accuracy: 0.8146\n",
      "Epoch 4 accuracy: 0.8925val_accuracy: 0.8524\n",
      "Epoch 5 accuracy: 0.9183val_accuracy: 0.8636\n",
      "Epoch 6 accuracy: 0.9375val_accuracy: 0.8602\n",
      "Epoch 7 accuracy: 0.9536val_accuracy: 0.8626\n",
      "Epoch 8 accuracy: 0.9666val_accuracy: 0.8638\n",
      "Epoch 9 accuracy: 0.9762val_accuracy: 0.8678\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train\n",
    "\n",
    "num_epochs = 10\n",
    "torch.manual_seed(1)\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f}'\n",
    "          f'val_accuracy: {acc_valid:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "OnDiskCache Exception: C:\\Users\\PipBoy3000/.cache\\torch\\text\\datasets\\IMDB\\aclImdb_v1\\test\\pos expected to be written by different process, but file is not ready in 300 seconds.\nThis exception is thrown by __iter__ of MapperIterDataPipe(datapipe=UnBatcherIterDataPipe, fn=functools.partial(<function _wait_promise_fn at 0x000001CEE37153A0>, 300), input_col=None, output_col=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m acc_test, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_test\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m      3\u001b[0m total_acc, total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text_batch, label_batch, lengths \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      6\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(text_batch, lengths)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, label_batch)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_typing.py:514\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\datapipe.py:344\u001b[0m, in \u001b[0;36m_IterDataPipeSerializationWrapper.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datapipe\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_typing.py:514\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\grouping.py:41\u001b[0m, in \u001b[0;36mShardingFilterIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_datapipe):\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_of_instances \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance_id:\n\u001b[0;32m     43\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_typing.py:514\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combinatorics.py:122\u001b[0m, in \u001b[0;36mShufflerIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[T_co]:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enabled:\n\u001b[1;32m--> 122\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatapipe:\n\u001b[0;32m    123\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m x\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_typing.py:514\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:115\u001b[0m, in \u001b[0;36mMapperIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[T_co]:\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatapipe:\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_fn(data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_typing.py:514\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\plain_text_reader.py:121\u001b[0m, in \u001b[0;36mLineReaderIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[Str_Or_Bytes, Tuple[\u001b[38;5;28mstr\u001b[39m, Str_Or_Bytes]]]:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path, file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_datapipe:\n\u001b[0;32m    122\u001b[0m         stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_helper\u001b[38;5;241m.\u001b[39mskip_lines(file)\n\u001b[0;32m    123\u001b[0m         stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_helper\u001b[38;5;241m.\u001b[39mstrip_newline(stream)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_typing.py:514\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\fileopener.py:68\u001b[0m, in \u001b[0;36mFileOpenerIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m get_file_binaries_from_pathnames(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatapipe, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\utils\\common.py:86\u001b[0m, in \u001b[0;36mget_file_binaries_from_pathnames\u001b[1;34m(pathnames, mode, encoding)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     84\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m mode\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pathname \u001b[38;5;129;01min\u001b[39;00m pathnames:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pathname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected string type for pathname, but got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m                         \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(pathname)))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_typing.py:514\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:51\u001b[0m, in \u001b[0;36mConcaterIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatapipes:\n\u001b[1;32m---> 51\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dp:\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_typing.py:514\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\filelister.py:58\u001b[0m, in \u001b[0;36mFileListerIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m] :\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatapipe:\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m get_file_pathnames_from_root(path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabspath, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_deterministic)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\_typing.py:514\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:116\u001b[0m, in \u001b[0;36mMapperIterDataPipe.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[T_co]:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatapipe:\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:81\u001b[0m, in \u001b[0;36mMapperIterDataPipe._apply_fn\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\cacheholder.py:300\u001b[0m, in \u001b[0;36m_wait_promise_fn\u001b[1;34m(timeout, filename)\u001b[0m\n\u001b[0;32m    298\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start \u001b[38;5;241m>\u001b[39m timeout:\n\u001b[1;32m--> 300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    301\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnDiskCache Exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected to be written by different process, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut file is not ready in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filename\n",
      "\u001b[1;31mException\u001b[0m: OnDiskCache Exception: C:\\Users\\PipBoy3000/.cache\\torch\\text\\datasets\\IMDB\\aclImdb_v1\\test\\pos expected to be written by different process, but file is not ready in 300 seconds.\nThis exception is thrown by __iter__ of MapperIterDataPipe(datapipe=UnBatcherIterDataPipe, fn=functools.partial(<function _wait_promise_fn at 0x000001CEE37153A0>, 300), input_col=None, output_col=None)"
     ]
    }
   ],
   "source": [
    "acc_test, _ = evaluate(test_dl)\n",
    "print(f'test_accuracy: {acc_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(69346, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim,\n",
    "                 rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n",
    "        self.relu= nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text, lenghts):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        _, (hidden, cell) = self.rnn(out)\n",
    "        out = torch.cat((hidden[-2, :, :],\n",
    "                         hidden[-1, :, :]), dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, \n",
    "            rnn_hidden_size, fc_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project two â€“ character-level language modeling in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
