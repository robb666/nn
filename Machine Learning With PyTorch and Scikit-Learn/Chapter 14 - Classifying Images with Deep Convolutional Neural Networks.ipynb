{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14 - Classifying Images with Deep Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d Implementation: [ 5. 14. 16. 26. 24. 34. 19. 22.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def conv1d(x, w, p=0, s=1):\n",
    "    w_rot = np.array(w[::-1])\n",
    "    x_padded = np.array(x)\n",
    "    if p > 0:\n",
    "        zero_pad = np.zeros(shape=p)\n",
    "        x_padded = np.concatenate([\n",
    "            zero_pad, x_padded, zero_pad\n",
    "        ])\n",
    "    res = []\n",
    "    for i in range(0, int((len(x_padded) - len(w_rot))) + 1, s):\n",
    "        res.append(np.sum(x_padded[i:i + w_rot.shape[0]] * w_rot))\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "## Testing:\n",
    "x = [1, 3, 2, 4, 5, 6, 1, 3]\n",
    "w = [1, 0, 3, 1, 2]\n",
    "\n",
    "print('Conv1d Implementation:',\n",
    "       conv1d(x, w, p=2, s=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Results: [ 5 14 16 26 24 34 19 22]\n"
     ]
    }
   ],
   "source": [
    "print('NumPy Results:',\n",
    "      np.convolve(x, w, mode='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing a discrete convolution in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Implementation:\n",
      " [[11. 25. 32. 13.]\n",
      " [19. 25. 24. 13.]\n",
      " [13. 28. 25. 17.]\n",
      " [11. 17. 14.  9.]]\n",
      "\n",
      "SciPy Results:\n",
      " [[11 25 32 13]\n",
      " [19 25 24 13]\n",
      " [13 28 25 17]\n",
      " [11 17 14  9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "\n",
    "def conv2d(X, W, p=(0, 0), s=(1, 1)):\n",
    "    W_rot = np.array(W)[::-1, ::-1]\n",
    "    X_orig = np.array(X)\n",
    "    n1 = X_orig.shape[0] + 2*p[0]\n",
    "    n2 = X_orig.shape[1] + 2*p[1]\n",
    "    X_padded = np.zeros(shape=(n1, n2))\n",
    "    X_padded[p[0]: p[0] + X_orig.shape[0],\n",
    "             p[1]: p[1] + X_orig.shape[1]] = X_orig\n",
    "    \n",
    "    res = []\n",
    "    for i in range(0,\n",
    "                   int((X_padded.shape[0] - W_rot.shape[0]) / s[0]) + 1, s[0]):\n",
    "        res.append([])\n",
    "        for j in range(0,\n",
    "                       int((X_padded.shape[1] - W_rot.shape[1]) / s[1]) + 1, s[1]):\n",
    "            X_sub = X_padded[i:i + W_rot.shape[0],\n",
    "                             j:j + W_rot.shape[1]]\n",
    "            res[-1].append(np.sum(X_sub * W_rot))\n",
    "            \n",
    "    return (np.array(res))\n",
    "    \n",
    "    \n",
    "    \n",
    "## Testing:\n",
    "X = [[1, 3, 2, 4], [5, 6, 1, 3], [1, 2, 0, 2], [3, 4, 3, 2]]\n",
    "W = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]\n",
    "\n",
    "\n",
    "print('Conv2d Implementation:\\n',\n",
    "      conv2d(X, W, p=(1, 1), s=(1, 1)))\n",
    "\n",
    "print()\n",
    "\n",
    "print('SciPy Results:\\n',\n",
    "      scipy.signal.convolve2d(X, W, mode='same'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# img = read_image('example-image.png')\n",
    "\n",
    "# print('Image shape:', img.shape)\n",
    "# print('Number of channels:', img.shape[0])\n",
    "# print('Image data type:', img.dtype)\n",
    "# print(img[:, 100:102, 100:102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "loss = loss_func(torch.tensor([0.9]), torch.tensor([1.0]))\n",
    "l2_lambda = 0.001\n",
    "conv_layer = nn.Conv2d(in_channels=3,\n",
    "                       out_channels=5,\n",
    "                       kernel_size=5)\n",
    "l2_penalty = l2_lambda * sum(\n",
    "        [(p**2).sum() for p in conv_layer.parameters()])\n",
    "\n",
    "loss_with_penalty = loss + l2_penalty\n",
    "linear_layer = nn.Linear(10, 16)\n",
    "\n",
    "l2_penalty = l2_lambda * sum([(p**2).sum() for p in linear_layer.parameters()])\n",
    "loss_with_penalty = loss + l2_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(\n",
    "#     model.parameters(),\n",
    "#     weight_decay=l2_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE (w Probas): 0.3711\n",
      "BCE (w Logits): 0.3711\n"
     ]
    }
   ],
   "source": [
    "####### Binary Cross-entropy\n",
    "\n",
    "logits = torch.tensor([0.8])\n",
    "probas = torch.sigmoid(logits)\n",
    "target = torch.tensor([1.0])\n",
    "bce_loss_fn = nn.BCELoss()\n",
    "bce_logits_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f'BCE (w Probas): {bce_loss_fn(probas, target):.4f}')\n",
    "print(f'BCE (w Logits): {bce_logits_loss_fn(logits, target):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCE (w Probas): 0.5996\n",
      "CCE (w Logits): 0.5996\n"
     ]
    }
   ],
   "source": [
    "####### Categorical Cross-entropy\n",
    "\n",
    "logits = torch.tensor([[1.5, 0.8, 2.1]])\n",
    "probas = torch.softmax(logits, dim=1)\n",
    "target = torch.tensor([2])\n",
    "cce_loss_fn = nn.NLLLoss()\n",
    "cce_logits_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f'CCE (w Probas): {cce_logits_loss_fn(logits, target):.4f}')\n",
    "print(f'CCE (w Logits): {cce_loss_fn(torch.log(probas), target):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a deep CNN using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\robert\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "image_path = './'\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_dataset = torchvision.datasets.MNIST(root=image_path, train=True,\n",
    "                                           transform=transform, download=True)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "mnist_valid_dataset = Subset(mnist_dataset,\n",
    "                             torch.arange(10000))\n",
    "\n",
    "mnist_train_dataset = Subset(mnist_dataset,\n",
    "                             torch.arange(\n",
    "                                 10000, len(mnist_dataset)\n",
    "                             ))\n",
    "\n",
    "mnist_test_dataset = torchvision.datasets.MNIST(\n",
    "    root=image_path, train=False,\n",
    "    transform=transform, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "train_dl = DataLoader(mnist_train_dataset,\n",
    "                      batch_size,\n",
    "                      shuffle=True)\n",
    "valid_dl = DataLoader(mnist_valid_dataset,\n",
    "                      batch_size,\n",
    "                      shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a CNN using the torch.nn module\n",
    "# Configuring CNN layers in PyTorch\n",
    "# Constructing a CNN in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module(\n",
    "    'conv1',\n",
    "    nn.Conv2d(\n",
    "        in_channels=1, out_channels=32,\n",
    "        kernel_size=5, padding=2\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('pool1', nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module(\n",
    "    'conv2',\n",
    "    nn.Conv2d(\n",
    "        in_channels=32, out_channels=64,\n",
    "        kernel_size=5, padding=2\n",
    "    )\n",
    ")\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('pool2', nn.MaxPool2d(kernel_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\robert\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 7, 7])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((4, 1, 28, 28))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3136])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add_module('flatten', nn.Flatten())\n",
    "x = torch.ones((4, 1, 28, 28))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module('fc1', nn.Linear(3136, 1024))\n",
    "model.add_module('relu3', nn.ReLU())\n",
    "model.add_module('dropout', nn.Dropout(p=0.5))\n",
    "model.add_module('fc2', nn.Linear(1024, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, train_dl, valid_dl):\n",
    "    loss_hist_train = [0] * num_epochs\n",
    "    accuracy_hist_train = [0] * num_epochs\n",
    "    loss_hist_valid = [0] * num_epochs\n",
    "    accuracy_hist_valid = [0] * num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            pred = model(x_batch)\n",
    "            loss = loss_fn(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_hist_train[epoch] += loss.item() * y_batch.size(0)\n",
    "            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "            accuracy_hist_train[epoch] += is_correct.sum()\n",
    "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_dl:\n",
    "                pred = model(x_batch)\n",
    "                loss = loss_fn(pred, y_batch)\n",
    "                loss_hist_valid[epoch] += loss.item() * y_batch.size(0)\n",
    "\n",
    "                is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "                accuracy_hist_valid[epoch]  += is_correct.sum()\n",
    "        loss_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch + 1} accuracy: '\n",
    "              f'{accuracy_hist_train[epoch]:.4f} val_accuracy: '\n",
    "              f'{accuracy_hist_valid[epoch]:.4f}')\n",
    "\n",
    "    return loss_hist_train, loss_hist_valid, \\\n",
    "           accuracy_hist_train, accuracy_hist_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "# num_epochs = 20\n",
    "# hist = train(model, num_epochs, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x_arr = np.arange(len(hist[0])) + 1\n",
    "# fig = plt.figure(figsize=(12, 4))\n",
    "# ax = fig.add_subplot(1, 2, 1)\n",
    "# ax.plot(x_arr, hist[0], '-o', label='Train loss')\n",
    "# ax.plot(x_arr, hist[1], '--<', label='Validation loss')\n",
    "\n",
    "# ax.legend(fontsize=15)\n",
    "# ax = fig.add_subplot(1, 2, 2)\n",
    "# ax.plot(x_arr, hist[2], '-o', label='Train acc.')\n",
    "# ax.plot(x_arr, hist[3], '--<', label='Validation acc.')\n",
    "\n",
    "# ax.legend(fontsize=15)\n",
    "# ax.set_xlabel('Epoch', size=15)\n",
    "# ax.set_ylabel('Accuracy', size=15)\n",
    "# plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = model(mnist_test_dataset.data.unsqueeze(1) / 225.)\n",
    "# is_correct = (torch.argmax(pred, dim=1) == mnist_test_dataset.targets).float()\n",
    "\n",
    "# print(f'Test accuracy: {is_correct.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(12, 4))\n",
    "# for i in range(12):\n",
    "#     ax = fig.add_subplot(2, 6, i+1)\n",
    "#     ax.set_xticks([]); ax.set_yticks([])\n",
    "#     img = mnist_test_dataset[i][0][0, :, :]\n",
    "#     pred = model(img.unsqueeze(1))\n",
    "#     y_pred = torch.argmax(pred)\n",
    "#     ax.imshow(img, cmap='gray_r')\n",
    "#     ax.text(0.9, 0.1, y_pred.item(),\n",
    "#             size=15, color='blue',\n",
    "#             horizontalalignment='center',\n",
    "#             verticalalignment='center',\n",
    "#             transform=ax.transAxes)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smile classification from face images using a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-819aa9f53dfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m celeba_train_dataset = torchvision.datasets.CelebA(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtarget_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'attr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\robert\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\datasets\\celeba.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             raise RuntimeError('Dataset not found or corrupted.' +\n\u001b[0m\u001b[0;32m     84\u001b[0m                                ' You can use download=True to download it')\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "image_path = './'\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='train',\n",
    "    target_type='attr', download=False\n",
    ")\n",
    "celeba_valid_dataset = torchvision.dataset.CelebA(\n",
    "    image_path, split='valid',\n",
    "    target_type='attr', download=False\n",
    ")\n",
    "celeba_test_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='test',\n",
    "    target_type='attr', download=False\n",
    ")\n",
    "\n",
    "print('Train set:', len(celeba_train_dataset))\n",
    "print('Validation set:', len(celeba_valid_dataset))\n",
    "print('Test set:', len(celeba_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8.5))\n",
    "## Column 1: cropping to a bounding box\n",
    "ax = fig.add_subplot(2, 5, 1)\n",
    "img, attr = celeba_train_dataset[0]\n",
    "ax.set_title('Crop to a \\nbounding-box', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 6)\n",
    "img_cropped = transforms.functional.crop(img, 50, 20, 128, 128)\n",
    "ax.imshow(img_cropped)\n",
    "\n",
    "## column 2: flipping (horizontally)\n",
    "ax = fig.add_subplot(2, 5, 2)\n",
    "img, attr = celeba_train_dataset[1]\n",
    "ax.set_title('Flip (horizontal)', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 7)\n",
    "img_flipped = transforms.functional.hflip(img)\n",
    "ax.imshow(img_flipped)\n",
    "\n",
    "## Column 3: adjust contrast\n",
    "ax = fig.add_subplot(2, 5, 3)\n",
    "img, attr = celeba_train_dataset[2]\n",
    "ax.set_title('Adjust contrast', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 8)\n",
    "img_adj_contrast = transforms.functional.adjust_contrast(\n",
    "    img, contrast_factor=2\n",
    ")\n",
    "ax.imshow(img_adj_contrast)\n",
    "\n",
    "## Column 4: adjust brightness\n",
    "ax = fig.add_subplot(2, 5, 4)\n",
    "img, attr = celeba_train_dataset[3]\n",
    "ax.set_title('Adjust brightness', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 9)\n",
    "img_adj_brightness = transforms.functional.adjust_brightness(\n",
    "    img, brightness_factor=1.3\n",
    ")\n",
    "ax.imshow(img_adj_brightness)\n",
    "\n",
    "## Column 5: cropping from image center\n",
    "ax = fig.add_subplot(2, 5, 5)\n",
    "img, attr = celeba_train_dataset[4]\n",
    "ax.set_title('Center crop\\nand resize', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 10)\n",
    "img_center_crop = transforms.functional.center_crop(\n",
    "    img, [0.7 * 218, 0.7 * 178]\n",
    ")\n",
    "img_resized = transforms.functional.resize(\n",
    "    img_center_crop, size=(218, 178)\n",
    ")\n",
    "ax.imshow(img_resized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
